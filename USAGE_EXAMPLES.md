# ğŸ“– ä½¿ç”¨ç¤ºä¾‹å¤§å…¨

## ğŸ¯ åŸºç¡€ä½¿ç”¨

### 1. Docker Run å‘½ä»¤æ¨¡æ¿

```bash
# æœ€ç®€å‘½ä»¤ï¼ˆå¿…é¡»é…ç½®ï¼‰
docker run -d \
  --name gemini-proxy \
  -p 8080:8080 \
  -e BEARER_TOKEN="YOUR_TOKEN" \
  -e CONFIG_ID="YOUR_CONFIG" \
  ghcr.io/yourusername/gemini-proxy:latest

# å®Œæ•´é…ç½®ï¼ˆæ¨èï¼‰
docker run -d \
  --name gemini-proxy \
  --restart unless-stopped \
  -p 8080:8080 \
  -e BEARER_TOKEN="eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9..." \
  -e CONFIG_ID="d06739ca-6683-46db-bb51-07395a392439" \
  -e PORT="8080" \
  -e DEBUG="false" \
  -e PROXY_URL="" \
  -e TZ=Asia/Shanghai \
  --log-driver json-file \
  --log-opt max-size=10m \
  --log-opt max-file=3 \
  ghcr.io/yourusername/gemini-proxy:latest
```

### 2. ç¯å¢ƒå˜é‡é…ç½®

```bash
# å¯¼å‡ºå˜é‡ï¼ˆæ¨èæ–¹å¼ï¼‰
export BEARER_TOKEN="your_token_here"
export CONFIG_ID="your_config_here"
export PORT="8080"
export DEBUG="false"

# è¿è¡Œæ—¶ç›´æ¥ä½¿ç”¨
docker run -d \
  -e BEARER_TOKEN \
  -e CONFIG_ID \
  -e PORT \
  -e DEBUG \
  ghcr.io/yourusername/gemini-proxy:latest
```

## ğŸŒ API è°ƒç”¨ç¤ºä¾‹

### 1. cURL æµå¼å“åº”

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer dummy" \
  -d '{
    "model": "gemini-2.5-flash",
    "messages": [
      {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç”¨ä¸­æ–‡ä»‹ç»ä¸€ä¸‹Python"}
    ],
    "stream": true,
    "temperature": 0.7,
    "max_tokens": 1000
  }'
```

### 2. cURL éæµå¼å“åº”

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-2.5-pro",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯åŠ©æ‰‹"},
      {"role": "user", "content": "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯Docker"}
    ],
    "stream": false,
    "temperature": 0.5
  }'
```

### 3. å¤šè½®å¯¹è¯

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-2.5-flash",
    "messages": [
      {"role": "user", "content": "ä½ å¥½"},
      {"role": "assistant", "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"},
      {"role": "user", "content": "æ¨èä¸€ä¸ªPythonå­¦ä¹ èµ„æº"}
    ],
    "stream": true
  }'
```

## ğŸ Python å®¢æˆ·ç«¯

### 1. OpenAI åº“ï¼ˆæ¨èï¼‰

```python
import openai

# é…ç½®
openai.api_base = "http://localhost:8080/v1"
openai.api_key = "dummy"  # ä»»æ„å€¼

# æµå¼å“åº”
def stream_chat():
    response = openai.ChatCompletion.create(
        model="gemini-2.5-flash",
        messages=[
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹Goè¯­è¨€"}
        ],
        stream=True,
        temperature=0.7
    )
    
    for chunk in response:
        if hasattr(chunk.choices[0].delta, 'content'):
            print(chunk.choices[0].delta.content, end="", flush=True)

# éæµå¼å“åº”
def normal_chat():
    response = openai.ChatCompletion.create(
        model="gemini-2.5-pro",
        messages=[
            {"role": "user", "content": "ä»€ä¹ˆæ˜¯å®¹å™¨åŒ–ï¼Ÿ"}
        ],
        stream=False
    )
    
    print(response.choices[0].message.content)

if __name__ == "__main__":
    stream_chat()
```

### 2. requests åº“

```python
import requests
import json

def chat_stream():
    url = "http://localhost:8080/v1/chat/completions"
    headers = {"Content-Type": "application/json"}
    data = {
        "model": "gemini-2.5-flash",
        "messages": [{"role": "user", "content": "ä½ å¥½"}],
        "stream": True
    }
    
    response = requests.post(url, headers=headers, json=data, stream=True)
    
    for line in response.iter_lines():
        if line:
            print(line.decode('utf-8'))

def chat_normal():
    url = "http://localhost:8080/v1/chat/completions"
    headers = {"Content-Type": "application/json"}
    data = {
        "model": "gemini-2.5-flash",
        "messages": [{"role": "user", "content": "ä½ å¥½"}],
        "stream": False
    }
    
    response = requests.post(url, headers=headers, json=data)
    print(response.json())
```

## ğŸ“± JavaScript/Node.js

### 1. OpenAI SDK

```javascript
const OpenAI = require('openai');

const openai = new OpenAI({
  baseURL: 'http://localhost:8080/v1',
  apiKey: 'dummy'
});

async function streamChat() {
  const stream = await openai.chat.completions.create({
    model: 'gemini-2.5-flash',
    messages: [{role: 'user', content: 'ä½ å¥½'}],
    stream: true
  });

  for await (const chunk of stream) {
    process.stdout.write(chunk.choices[0]?.delta?.content || '');
  }
}

async function normalChat() {
  const response = await openai.chat.completions.create({
    model: 'gemini-2.5-pro',
    messages: [{role: 'user', content: 'ä»€ä¹ˆæ˜¯Kubernetes'}],
    stream: false
  });

  console.log(response.choices[0].message.content);
}

streamChat();
```

### 2. Fetch API

```javascript
async function chatStream() {
  const response = await fetch('http://localhost:8080/v1/chat/completions', {
    method: 'POST',
    headers: {'Content-Type': 'application/json'},
    body: JSON.stringify({
      model: 'gemini-2.5-flash',
      messages: [{role: 'user', content: 'ä½ å¥½'}],
      stream: true
    })
  });

  const reader = response.body.getReader();
  const decoder = new TextDecoder();
  
  while (true) {
    const {done, value} = await reader.read();
    if (done) break;
    console.log(decoder.decode(value));
  }
}
```

## ğŸ¨ é«˜çº§ç”¨æ³•

### 1. ç³»ç»Ÿæç¤ºè¯

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-2.5-flash",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonç¼–ç¨‹åŠ©æ‰‹ï¼Œåªç”¨ä¸­æ–‡å›ç­”ï¼Œä»£ç è¦è¯¦ç»†æ³¨é‡Š"},
      {"role": "user", "content": "å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•"}
    ],
    "stream": true
  }'
```

### 2. å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-2.5-flash",
    "messages": [
      {"role": "user", "content": "Dockeræ˜¯ä»€ä¹ˆï¼Ÿ"},
      {"role": "assistant", "content": "Dockeræ˜¯ä¸€ä¸ªå®¹å™¨åŒ–å¹³å°..."},
      {"role": "user", "content": "å®ƒå’Œè™šæ‹Ÿæœºæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"}
    ],
    "stream": true
  }'
```

### 3. å‚æ•°è°ƒä¼˜

```bash
# ä½æ¸©åº¦ï¼ˆæ›´ç¡®å®šæ€§ï¼‰
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-2.5-flash",
    "messages": [{"role": "user", "content": "ä½ å¥½"}],
    "temperature": 0.1,
    "stream": true
  }'

# é«˜æ¸©åº¦ï¼ˆæ›´æœ‰åˆ›æ„ï¼‰
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-2.5-flash",
    "messages": [{"role": "user", "content": "å†™ä¸€é¦–è¯—"}],
    "temperature": 1.5,
    "stream": true
  }'
```

## ğŸ”§ å·¥å…·å‡½æ•°

### 1. å¥åº·æ£€æŸ¥

```bash
curl http://localhost:8080/health
# è¿”å›: {"status":"ok","timestamp":"2026-01-14T10:00:00Z"}
```

### 2. è·å–æ¨¡å‹åˆ—è¡¨

```bash
curl http://localhost:8080/v1/models
# è¿”å›: {"object":"list","data":[...]}
```

### 3. æµ‹è¯•è¿æ¥

```bash
# å¿«é€Ÿæµ‹è¯•
curl -s http://localhost:8080/health | jq .

# å®Œæ•´æµ‹è¯•
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"gemini-2.5-flash","messages":[{"role":"user","content":"test"}],"stream":false}' \
  | jq .
```

## ğŸ“Š æ—¥å¿—åˆ†æ

### 1. æŸ¥çœ‹è¯·æ±‚æ—¥å¿—

```bash
# å®æ—¶æ—¥å¿—
docker logs -f gemini-proxy 2>&1 | grep "Request"

# ç»Ÿè®¡è¯·æ±‚æ¬¡æ•°
docker logs gemini-proxy 2>&1 | grep "Request" | wc -l

# æŸ¥çœ‹é”™è¯¯
docker logs gemini-proxy 2>&1 | grep "ERROR"
```

### 2. æ€§èƒ½ç›‘æ§

```bash
# æŸ¥çœ‹å“åº”æ—¶é—´
docker logs -f gemini-proxy 2>&1 | grep "duration"

# æŸ¥çœ‹èµ„æºå ç”¨
docker stats gemini-proxy
```

## ğŸš¨ æ•…éšœæ’æŸ¥

### 1. è®¤è¯å¤±è´¥

```bash
# æ£€æŸ¥ Token
echo $BEARER_TOKEN | head -c 50

# é‡æ–°è·å– Token
# ä»æµè§ˆå™¨ Network æ ‡ç­¾å¤åˆ¶
```

### 2. é…ç½®é”™è¯¯

```bash
# æŸ¥çœ‹ç¯å¢ƒå˜é‡
docker exec gemini-proxy env

# æ£€æŸ¥é…ç½®
docker logs gemini-proxy | grep "Config"
```

### 3. ç½‘ç»œé—®é¢˜

```bash
# æµ‹è¯• API è¿é€šæ€§
curl -I https://biz-discoveryengine.googleapis.com

# æ£€æŸ¥å®¹å™¨ç½‘ç»œ
docker exec gemini-proxy ping -c 3 8.8.8.8
```

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

```bash
# ä½¿ç”¨ docker-compose
docker-compose up -d

# è®¾ç½®é‡å¯ç­–ç•¥
docker run -d --restart unless-stopped ...

# é…ç½®æ—¥å¿—è½®è½¬
docker run -d \
  --log-driver json-file \
  --log-opt max-size=10m \
  --log-opt max-file=3 \
  ...
```

### 2. å®‰å…¨é…ç½®

```bash
# ä¸è¦åœ¨å‘½ä»¤è¡Œæš´éœ² Token
export BEARER_TOKEN="your_token"
docker run -e BEARER_TOKEN ...

# ä½¿ç”¨ secretsï¼ˆDocker Swarmï¼‰
echo "your_token" | docker secret create bearer_token -
docker run --secret bearer_token ...
```

### 3. æ€§èƒ½ä¼˜åŒ–

```bash
# é™åˆ¶èµ„æº
docker run -d \
  --memory=512m \
  --cpus=1.0 \
  ...
```

---

**æç¤º**: æ‰€æœ‰ç¤ºä¾‹ä¸­çš„ `YOUR_TOKEN` å’Œ `YOUR_CONFIG` éœ€è¦æ›¿æ¢ä¸ºå®é™…å€¼ã€‚